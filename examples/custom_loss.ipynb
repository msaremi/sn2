{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Custom Loss\n",
    "The SNÂ² Solver tries to minimize the distance between the sample covariance\n",
    "and the covariance induced by the model. There are more than one way to compute\n",
    "this distance.\n",
    "\n",
    "We will go through different methods for computing the distance between the sample covariance and the induced one.\n",
    "These functions will be called the loss functions.\n",
    "Let's start by taking the example from the [introduction](introduction.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import torch\n",
    "from sn2_cuda import SN2Solver\n",
    "from sn2_cuda.loss import LossBase, KullbackLeibler, Bhattacharyya, SquaredHellinger\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "struct = torch.tensor([\n",
    "        [1, 1, 1, 0],\n",
    "        [0, 1, 0, 1],\n",
    "        [1, 0, 0, 0],\n",
    "        [0, 1, 0, 0],\n",
    "        [0, 0, 1, 0],\n",
    "        [0, 0, 0, 1],\n",
    "        [0, 1, 1, 1],  # V_X\n",
    "        [0, 0, 1, 0],  # V_BP\n",
    "        [0, 0, 0, 1],  # V_BMI\n",
    "        [0, 0, 0, 0],  # V_Y\n",
    "    ], dtype=torch.bool)\n",
    "\n",
    "sample_covariance = torch.tensor([\n",
    "        [2,  3,  6,  8],\n",
    "        [3,  7, 12, 16],\n",
    "        [6, 12, 23, 30],\n",
    "        [8, 16, 30, 41],\n",
    "    ], device=device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Before anything, we encapsulate our training code for multiple use."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-07-19T01:00:58.325666300Z",
     "start_time": "2025-07-19T00:58:41.145088Z"
    }
   },
   "source": [
    "def parametrize(pmDAG, max_iterations, min_error):\n",
    "    optim = torch.optim.Adamax([pmDAG.weights], lr=0.001)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        pmDAG.forward()\n",
    "        error = pmDAG.loss().item()\n",
    "\n",
    "        if error < min_error:\n",
    "            break\n",
    "\n",
    "        pmDAG.backward()\n",
    "        optim.step()\n",
    "\n",
    "        if i % (max_iterations / 10) == 0:\n",
    "            print(f\"iteration={i:<10} loss={error:<15.5}\")\n",
    "    else:\n",
    "        print(\"Did not converge in the maximum number of iterations!\")"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We also configure the parametrization options."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "max_iterations = 10000\n",
    "min_error = 1.0e-7"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-07-19T01:00:58.326666Z",
     "start_time": "2025-07-19T00:56:53.133486Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use different built-in loss functions, namely `KullbackLeibler`, `Bhattacharyya`, and `SquaredHellinger`.\n",
    "`KullbackLeibler` is the default loss function and the pmDAG parametrized using this loss function will be the\n",
    "maximum likelihood estimation of the pmDAG."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pmDAG = SN2Solver(\n",
    "    struct,\n",
    "    sample_covariance=sample_covariance,\n",
    "    loss=KullbackLeibler()\n",
    ")\n",
    "\n",
    "parametrize(pmDAG, max_iterations=max_iterations, min_error=min_error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-07-19T01:00:58.327660800Z",
     "start_time": "2025-07-19T00:56:53.172075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0          loss=9.1502         \n",
      "iteration=1000       loss=0.44752        \n",
      "iteration=2000       loss=0.13949        \n",
      "iteration=3000       loss=0.012807       \n",
      "iteration=4000       loss=2.3842e-06     \n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "We could use the `Bhattacharyya` loss function, and it would give us the same paramterized pmDAG.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pmDAG = SN2Solver(\n",
    "    struct,\n",
    "    sample_covariance=sample_covariance,\n",
    "    loss=Bhattacharyya()\n",
    ")\n",
    "\n",
    "parametrize(pmDAG, max_iterations=max_iterations, min_error=min_error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-07-19T01:00:58.329656300Z",
     "start_time": "2025-07-19T00:59:32.037726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0          loss=0.77869        \n",
      "iteration=1000       loss=0.72944        \n",
      "iteration=2000       loss=0.6818         \n",
      "iteration=3000       loss=0.64291        \n",
      "iteration=4000       loss=0.60844        \n",
      "iteration=5000       loss=0.57685        \n",
      "iteration=6000       loss=0.5496         \n",
      "iteration=7000       loss=0.52818        \n",
      "iteration=8000       loss=0.51237        \n",
      "iteration=9000       loss=0.50097        \n",
      "Did not converge in the maximum number of iterations!\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "This prints the same covariance matrix as before.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(pmDAG.visible_covariance_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-07-19T01:00:58.330655200Z",
     "start_time": "2025-07-19T00:57:45.298965Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 110.9302,   61.3989,  188.5205,  301.2141],\n",
      "        [  61.3989,  138.2396,  248.1375,  307.7289],\n",
      "        [ 188.5205,  248.1375,  662.2336,  849.6810],\n",
      "        [ 301.2141,  307.7289,  849.6810, 1371.0049]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Custom Loss Function\n",
    "\n",
    "When non of the two built-in custom functions are desirable, one could simply define an arbitrary custom loss function.\n",
    "This is done by subclassing the `LossBase` class.\n",
    "\n",
    "The signature of the loss function is as follows. We use the methods that give the\n",
    "Kullback-Leibler divergence from the sample covariange. However, these methods are arbitrary."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MyLoss(LossBase):\n",
    "    def loss_proxy(self, visible_covariance):\n",
    "        return torch.trace(self.sample_covariance_inv @ visible_covariance) - torch.logdet(visible_covariance)\n",
    "\n",
    "    def loss(self, visible_covariance):\n",
    "        return (self.loss_proxy(visible_covariance) - self.size + self.sample_covariance_logdet) / 2\n",
    "\n",
    "    def loss_backward(self, visible_covariance, visible_covariance_grad):\n",
    "        visible_covariance_grad.copy_(self.sample_covariance_inv)\n",
    "        visible_covariance_grad.subtract_(torch.inverse(visible_covariance))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-07-19T01:00:58.331653200Z",
     "start_time": "2025-07-19T00:57:45.411906Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notice that there are three functions to be overloaded: `loss`, `loss_proxy`, and `loss_backward`.\n",
    "- `loss` computes the actual distance between the sample covariance and the induced covariance.\n",
    "- `loss_proxy` is a computationally more effective function that is a monotonic counterpart of the `loss` function\n",
    "- `loss_backward` computes the derivative of the distance with respect t the induced covariance.\n",
    "\n",
    "Among these functions, only `loss_backward` is required for the parametrization to work. The other two can be used\n",
    "to get the loss or loss proxy values when one desires.\n",
    "\n",
    "Now, let's re-parametrize the pmDAG using this new custom function."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pmDAG = SN2Solver(\n",
    "    struct,\n",
    "    sample_covariance=sample_covariance,\n",
    "    loss=MyLoss()\n",
    ")\n",
    "\n",
    "parametrize(pmDAG, max_iterations=max_iterations, min_error=min_error)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-07-19T01:00:58.345619500Z",
     "start_time": "2025-07-19T00:57:45.460190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0          loss=3.3356         \n",
      "iteration=1000       loss=0.86539        \n",
      "iteration=2000       loss=0.33636        \n",
      "iteration=3000       loss=0.003971       \n",
      "iteration=4000       loss=5.6386e-05     \n",
      "iteration=5000       loss=4.4107e-06     \n",
      "iteration=6000       loss=0.00042236     \n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you notice, this gives similar convergence results to our initial `KullbackLeibler` loss function.\n",
    "Now, let's print the resulting visible covariance matrix."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(pmDAG.visible_covariance_)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-07-19T01:00:58.346624700Z",
     "start_time": "2025-07-19T00:57:59.047950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.0008,  2.9985,  5.9992,  7.9986],\n",
      "        [ 2.9985,  6.9926, 11.9895, 15.9849],\n",
      "        [ 5.9992, 11.9895, 22.9867, 29.9804],\n",
      "        [ 7.9986, 15.9849, 29.9804, 40.9718]], device='cuda:0')\n"
     ]
    }
   ],
   "execution_count": 9
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-cba583ec",
   "language": "python",
   "display_name": "PyCharm (An-AI-Chatbot-in-Python-and-Flask-main)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
